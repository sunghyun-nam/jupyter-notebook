{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 2], name=\"x\")\n",
    "Y = tf.placeholder(tf.float32, [None, 1], name=\"y\")\n",
    "W = tf.Variable(tf.random_normal([2, 1]), name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = tf.Variable(tf.random_uniform([2, 5], -1.0, 1.0), name = 'Weight1')    \n",
    "W2 = tf.Variable(tf.random_uniform([5, 5], -1.0, 1.0), name = 'Weight2')   \n",
    "W3 = tf.Variable(tf.random_uniform([5, 5], -1.0, 1.0), name = 'Weight3')\n",
    "W4 = tf.Variable(tf.random_uniform([5, 5], -1.0, 1.0), name = 'Weight4')\n",
    "W5 = tf.Variable(tf.random_uniform([5, 5], -1.0, 1.0), name = 'Weight5')\n",
    "W6 = tf.Variable(tf.random_uniform([5, 5], -1.0, 1.0), name = 'Weight6')\n",
    "W7 = tf.Variable(tf.random_uniform([5, 5], -1.0, 1.0), name = 'Weight7')\n",
    "W8 = tf.Variable(tf.random_uniform([5, 5], -1.0, 1.0), name = 'Weight8')\n",
    "W9 = tf.Variable(tf.random_uniform([5, 5], -1.0, 1.0), name = 'Weight9')\n",
    "W10 = tf.Variable(tf.random_uniform([5, 5], -1.0, 1.0), name = 'Weight10')\n",
    "W11 = tf.Variable(tf.random_uniform([5, 1], -1.0, 1.0), name = 'Weight11')\n",
    "# input layer과 output layer만 신경쓰면 됨, hidden layer는 숫자만 맞추면 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = tf.Variable(tf.zeros([5]), name = 'Bias1')\n",
    "b2 = tf.Variable(tf.zeros([5]), name = 'Bias2')\n",
    "b3 = tf.Variable(tf.zeros([5]), name = 'Bias3')\n",
    "b4 = tf.Variable(tf.zeros([5]), name = 'Bias4')\n",
    "b5 = tf.Variable(tf.zeros([5]), name = 'Bias5')\n",
    "b6 = tf.Variable(tf.zeros([5]), name = 'Bias6')\n",
    "b7 = tf.Variable(tf.zeros([5]), name = 'Bias7')\n",
    "b8 = tf.Variable(tf.zeros([5]), name = 'Bias8')\n",
    "b9 = tf.Variable(tf.zeros([5]), name = 'Bias9')\n",
    "b10 = tf.Variable(tf.zeros([5]), name = 'Bias10')\n",
    "b11 = tf.Variable(tf.zeros([1]), name = 'Bias11')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"layer1\") as scope:\n",
    "    L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "with tf.name_scope(\"layer2\") as scope:\n",
    "    L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "with tf.name_scope(\"layer3\") as scope:\n",
    "    L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
    "with tf.name_scope(\"layer4\") as scope:\n",
    "    L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
    "with tf.name_scope(\"layer5\") as scope:\n",
    "    L5 = tf.nn.relu(tf.matmul(L4, W5) + b5)\n",
    "with tf.name_scope(\"layer6\") as scope:\n",
    "    L6 = tf.nn.relu(tf.matmul(L5, W6) + b6)\n",
    "with tf.name_scope(\"layer7\") as scope:\n",
    "    L7 = tf.nn.relu(tf.matmul(L6, W7) + b7)\n",
    "with tf.name_scope(\"layer8\") as scope:\n",
    "    L8 = tf.nn.relu(tf.matmul(L7, W8) + b8)\n",
    "with tf.name_scope(\"layer9\") as scope:\n",
    "    L9 = tf.nn.relu(tf.matmul(L8, W9) + b9)\n",
    "with tf.name_scope(\"layer10\") as scope:\n",
    "    L10 = tf.nn.relu(tf.matmul(L9, W10) + b10)\n",
    "\n",
    "with tf.name_scope(\"test\") as scope:\n",
    "    hypothesis = tf.sigmoid(tf.matmul(L10, W11) + b11)   # 마지막은 sigmoid 사용 출력 값은 0 ~ 1 값이어야 하므로  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor 'accuracy_7:0' shape=() dtype=string>"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "with tf.name_scope(\"Cost\"):\n",
    "    cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "    tf.summary.scalar(\"Cost\", cost)\n",
    "with tf.name_scope(\"Train\"):\n",
    "    train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "tf.summary.scalar(\"accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 0.69633245 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "100 0.09062396 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "200 0.011592399 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "300 0.0037467815 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "400 0.0019140658 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "500 0.001196452 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "600 0.0008352186 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "700 0.0006261384 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "800 0.00049006904 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "900 0.0003976801 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "1000 0.0003314293 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "1100 0.0002818845 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "1200 0.00024400213 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "1300 0.00021388897 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "1400 0.00018968042 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "1500 0.00016966148 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "1600 0.00015313114 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "1700 0.00013919477 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "1800 0.00012734543 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "1900 0.000117120915 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "2000 0.00010817834 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "2100 0.000100398436 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "2200 9.357248e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "2300 8.752159e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "2400 8.205201e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "2500 7.7178614e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "2600 7.278216e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "2700 6.881793e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "2800 6.52114e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "2900 6.193276e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "3000 5.8952195e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "3100 5.6150482e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "3200 5.3602125e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "3300 5.1262417e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "3400 4.9086644e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "3500 4.708971e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "3600 4.5212e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "3700 4.3483324e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "3800 4.1814266e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "3900 4.029423e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "4000 3.889342e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "4100 3.7492617e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "4200 3.6240832e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "4300 3.506356e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "4400 3.3916098e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "4500 3.287295e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "4600 3.182981e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "4700 3.0935687e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "4800 3.0041569e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "4900 2.9147452e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "5000 2.8342743e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "5100 2.7597645e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "5200 2.6822749e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "5300 2.610746e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "5400 2.5481579e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "5500 2.4796096e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "5600 2.420002e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "5700 2.3603949e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "5800 2.3037683e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "5900 2.2501219e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "6000 2.1994558e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "6100 2.1487896e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "6200 2.1011041e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "6300 2.0534186e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "6400 2.0116939e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "6500 1.9699688e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "6600 1.9282441e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "6700 1.8865197e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "6800 1.8477753e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "6900 1.8120114e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "7000 1.7792278e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "7100 1.7434639e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "7200 1.7106802e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "7300 1.6808768e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "7400 1.6480932e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "7500 1.6182901e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "7600 1.5884869e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "7700 1.5616639e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "7800 1.534841e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "7900 1.50801825e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "8000 1.4841758e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "8100 1.457353e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "8200 1.4335104e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "8300 1.409668e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "8400 1.3858256e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "8500 1.3649633e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "8600 1.3441013e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "8700 1.3232391e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "8800 1.30237695e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "8900 1.2844952e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "9000 1.26661325e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "9100 1.2457513e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "9200 1.2278695e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "9300 1.2070073e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "9400 1.192106e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "9500 1.17422405e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "9600 1.1593225e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "9700 1.1444212e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "9800 1.1295197e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "9900 1.1116379e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "10000 1.0967364e-05 [[-1.3202312 ]\n",
      " [ 0.77005357]]\n",
      "\n",
      "Hypothesis:  [[2.0925985e-05]\n",
      " [9.9999869e-01]\n",
      " [9.9999928e-01]\n",
      " [2.0926303e-05]] \n",
      "Correct:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 100 == 0:\n",
    "                print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run(W))  # W로 하나 W2로 하나 차이 없음.\n",
    "\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}